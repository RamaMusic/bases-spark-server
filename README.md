# Spark Loader

Un entorno de ejecuciÃ³n ultra-rÃ¡pido para PySpark que mantiene los datos en memoria y recarga tu cÃ³digo al vuelo.

## Quick Start ğŸš€

### 1. Levantar el Servidor
Construye la imagen y espera a que Spark estÃ© listo:

```bash
docker-compose up --build --wait -d
```

### 2. Ejecutar tu CÃ³digo
Corre el script que le avisa al servidor que procese `lab.py`:

```bash
docker exec -it spark python3 submit.py
```

## ğŸ“‚ Archivos Importantes

Todo lo que tenÃ©s que tocar estÃ¡ en la carpeta `dataset/`:

*   **`dataset/lab.py`**: **Â¡EDITÃ ESTE ARCHIVO!** AcÃ¡ escribÃ­s tu lÃ³gica de PySpark.
*   **`dataset/`**: PonÃ© tus archivos de datos (CSVs, TXTs) acÃ¡.
    *   *Nota*: Desde el cÃ³digo, accedÃ©s a estos archivos en `/bases/` (ej: `/bases/data.csv`).

---

## Â¿CÃ³mo funciona?

El sistema separa la **carga de datos** de la **lÃ³gica de anÃ¡lisis**:

1.  **`server.py` (El Motor)**: Arranca Spark, carga el CSV en un DataFrame/RDD y los deja en cachÃ©. Luego se queda escuchando en el puerto `9999`.
2.  **`lab.py` (Tu CÃ³digo)**: AcÃ¡ escribÃ­s tu lÃ³gica. El servidor recarga este archivo cada vez que lo llamÃ¡s, inyectÃ¡ndole las variables `spark`, `sc`, `df` y `rdd` ya listas.
3.  **`submit.py` (El Gatillo)**: Se conecta al servidor y le dice "Â¡EjecutÃ¡ ahora!". Recibe la salida y te la muestra.

## Flujo de EjecuciÃ³n ğŸ”„

```mermaid
sequenceDiagram
    participant Dev as ğŸ‘©â€ğŸ’» Vos
    participant Submit as ğŸš€ submit.py
    participant Server as ğŸ¢ server.py (Persistente)
    participant Lab as ğŸ§ª lab.py (Recargable)

    Note over Server: 1. Inicia Spark<br/>2. Carga & Cachea Datos<br/>3. Espera en puerto 9999...
    
    Dev->>Lab: Guardas cambios
    Dev->>Submit: Ejecutas
    Submit->>Server: Conecta
    Server->>Lab: reload(lab)
    Server->>Lab: run(spark, sc, df, rdd)
    Lab-->>Server: Output (prints)
    Server-->>Submit: EnvÃ­a Output
    Submit-->>Dev: Ves el resultado
```

## Comentarios Importantes

*   **No reinicies Docker**: La gracia es que Spark siga corriendo. Solo editÃ¡ `lab.py` y corrÃ© `submit.py`.
*   **Variables Disponibles**: En `lab.py` ya tenÃ©s:
    *   `spark`: La sesiÃ³n de Spark.
    *   `sc`: El contexto de Spark.
    *   `df`: El DataFrame con tus datos cargados.
    *   `rdd`: El RDD con tus datos cargados.
*   **Manejo de Errores**: Si tu cÃ³digo falla, el servidor atrapa la excepciÃ³n y te muestra el traceback sin crashear el contenedor.
